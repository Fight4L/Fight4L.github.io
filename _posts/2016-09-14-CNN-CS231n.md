---
layout: post
title: "CNN CS231n"
date: 2016-09-14 11:02
author: Liu
category: Computer Vision
tags: UTS Sydney CV
finished: true
---

> 老师提供的阅读材，上周就应该看完，但还是没看，这周需要看完然后看两篇FNN的论文并做一个report。 材料来自[__CS231n__](http://cs231n.github.io/convolutional-networks/)
PS:才发现有[__翻译__](https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit)，真真好人，我真真浪费时间t-T
PPS：再也不干像一句一句翻译这么笨蛋的方法了。学习还是理解为主。以下仅为总结的笔记。

## 神经网络

神经网络是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型用于对函数进行估计或近似。

[如何简单形象又有趣地讲解神经网络是什么？](https://www.zhihu.com/question/22553761)

神经元就是当h大于0时输出1，h小于0时输出0这么一个模型，它的实质就是把特征空间一切两半，认为两半分别属两个类。

多层神经网络中底层神经元的输出是高层神经元的输入。神经网络神奇的地方在于它的每一个组件非常简单——把空间切一刀+某种激活函数(0-1阶跃、sigmoid、max-pooling)，但是可以一层一层级联。输入向量连到许多神经元上，这些神经元的输出又连到一堆神经元上，这一过程可以重复很多次。

神经网络的训练依靠反向传播算法：最开始输入层输入特征向量，网络层层计算获得输出，输出层发现输出和正确的类号不一样，这时它就让最后一层神经元进行参数调整，最后一层神经元不仅自己调整参数，还会勒令连接它的倒数第二层神经元调整，层层往回退着调整。经过调整的网络会在样本上继续测试，如果输出还是老分错，继续来一轮回退调整，直到网络输出满意为止。

> 以下内容来自
[神经网络：文艺女vs理工男](https://zhuanlan.zhihu.com/p/20265859)
[卷积神经网络](http://blog.csdn.net/stdcoutzyx/article/details/41596663)
[神经网络浅讲：从神经元到深度学习](http://www.cnblogs.com/subconscious/p/5058741.html)__(必看！)__
[ 神经网络学习之M-P模型](http://blog.csdn.net/u013007900/article/details/50066315)

### 神经网络和数学模型MP

1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP。

MP模型是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型。

生物神经网络的假定特点：

1. 每个神经元都是一个多输入单输出的信息处理单元；
2. 神经元输入分兴奋性输入和抑制性输入两种类型；
3. 神经元具有空间整合特性和阈值特性；
4. 神经元输入与输出间有固定的时滞，主要取决于突触延搁

按照生物神经元，建立M-P模型。神经元可以表示如下：

![神经元](/img/blog/20160914/1.png)

结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的权重（其值称为权值），这是需要训练得到的。在神经元模型里，每个有向箭头表示的是值的加权传递。我们使用a来表示输入，用w来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是a，端中间有加权参数w，经过这个加权后的信号会变成a\*w，因此在连接的末端，信号的大小就变成了a\*w。  

将神经元图中的所有变量用符号表示，那么对应的公式为 ![神经元](/img/blog/20160914/2.png)

但只输出0和1，不可微，不利于数学分析，实际过程中使用sgn函数(即函数f)等。

1943年发布的MP模型，虽然简单，但已经建立了神经网络大厦的地基。但是，MP模型中，权重的值都是__预先设置__的，因此不能学习。

### 感知器

1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字--“感知器”（Perceptron）。

在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变。

![单层神经网络](/img/blog/20160914/1.jpg)

在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。

我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。

假如我们要预测的目标不再是一个值，而是一个向量。那么可以在输出层再增加一个“输出单元”。

下图显示了带有两个输出单元的单层神经网络，其中输出单元h1和h2的计算公式如下图。

![单层神经网络扩展](/img/blog/20160914/9.jpg)

使用二维的下标，用w(x ,y)来表达一个权值。下标中的x代表后一层神经元的序号，而y代表前一层神经元的序号（序号的顺序从上到下）。

可以用矩阵乘法来表达这两个公式,输出公式可以改写成：f(W \* a) = h;这个公式就是神经网络中从前一层计算后一层的__矩阵运算__。

与神经元模型不同，感知器中的权值是通过__训练__得到的.感知器只能做简单的线性分类任务。

### 两层神经网络(多层感知器)

单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。

两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。

![两层神经网络](/img/blog/20160914/10.jpg)

使用矩阵运算，即
- f(W(1) * a(1)) = a(2);
- f(W(2) * a(2)) = h;

假设我们的预测目标是一个向量，那么与前面类似，只需要在“输出层”再增加节点即可。

需要说明的是，神经网络的结构图中一直默认存在着偏置节点（bias unit）。它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量b，称之为偏置。

在考虑了偏置以后的一个神经网络的矩阵运算如下：

- f(W(1) * a(1) + b(1)) = a(2);
- f(W(2) * a(2) + b(2)) = h;

在两层神经网络中，我们不再使用sgn函数作为函数f，而是使用平滑函数sigmoid作为函数f。我们把函数f也称作__激活函数__（active function）。

__事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。__

与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。

多个神经元组成神经网络模型

![NN](/img/blog/20160914/4.jpg)

除了输入、输出两层，其它的叫隐藏层。每个神经元是一个决策单元，整个神经网络是一个复杂的决策网络。它要做的事，就是通过隐藏层，把输入层处理成我们想要的输出层。

下图为一个具有一个隐藏层的神经网络

![NN](/img/blog/20160914/3.png)

可以扩展到多层

我们要做就是给神经网络里面的各个神经元配置合适的权重矢量和阀值，这个过程就叫神经网络的“训练”。
神经网络的训练方法也同Logistic类似，不过由于其多层性，还需要利用链式求导法则对隐含层的节点进行求导，即梯度下降+链式求导法则，专业名称为反向传播。


构建目标函数来衡量神经网络的输出和我们的预期差距：

![Object Function](/img/blog/20160914/6.jpg)

神经网络的输出和预期越接近， 就越接近于0……“训练”过程就是选择合适的w和b，让C(w,b)最小化

常用的方法是：__梯度下降__

![梯度下降](/img/blog/20160914/7.jpg)

前面定义的C因为是对所有样本的全平均，算起来很麻烦。所以我们又采取了一种偷懒的方式——抽样：

![随机梯度下降](/img/blog/20160914/8.jpg)

即__随机梯度下降__。



## 卷积神经网络

卷积神经网络Convolutional Neural Networks，又叫CNNs或者ConvNets。卷积神经网络是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。

卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。

- 卷积层

  卷积层（Convolutional layer），卷积神经网络中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。

- 线性整流层

  线性整流层（Rectified Linear Units layer, ReLU layer），这一层神经的活性化函数（Activation function）使用线性整流（Rectified Linear Units, ReLU） f(x)=max(0,x)。

- 池化层

  池化层（Pooling layer），通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。

与传统神经网络采用全连接的方式不同，CNN通过__局部连接__和__权值共享__的方法来避免参数过多。

## [卷积神经网络 参考1](http://blog.csdn.net/stdcoutzyx/article/details/41596663)

## [卷积神经网络 参考2](http://www.jeyzhang.com/cnn-learning-notes-1.html)
